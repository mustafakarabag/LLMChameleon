{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c06eac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Set to the GPU you want to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7b90c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import datetime\n",
    "import traceback\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import pickle\n",
    "import collections\n",
    "import itertools\n",
    "from copy import deepcopy\n",
    "from functools import partial\n",
    "import dill\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import lovely_tensors as lt\n",
    "lt.monkey_patch()\n",
    "\n",
    "from llmg.utils.mix import seed_all\n",
    "from llmg.chameleon.utils import (\n",
    "    load_game_logs,\n",
    "    construct_result_dict,\n",
    "    collect_llm_preds_at_breakpoints,\n",
    ")\n",
    "from llmg.chameleon.NaturalLanguageTalker.naturallanguagetalker import NaturalLanguageTalker\n",
    "from llmg.chameleon.NaturalLanguageTalker.OpenaiTalker import OpenaiTalker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11ed803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-collected data\n",
    "data_dirs = [\n",
    "    os.path.join(os.environ[\"DATA_DIR\"], \"chameleon\", \"2025-08-13_20-15\"),\n",
    "]\n",
    "\n",
    "game_logs = load_game_logs(\n",
    "    data_dirs,\n",
    "    layer_to_probe=None,\n",
    "    token_to_probe=None,\n",
    "    max_games=100,\n",
    "    verbose=2,\n",
    ")\n",
    "all_games = construct_result_dict(game_logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4909205",
   "metadata": {},
   "source": [
    "## Manual analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847d0cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check failed games\n",
    "for g in game_logs:\n",
    "    if g[\"game_result\"] == \"Fail\":\n",
    "        print(g[\"explanation\"])\n",
    "        print(g[\"chameleon_response\"])\n",
    "        print(g[\"possible_words\"])\n",
    "        for ms in g[\"messages\"]:\n",
    "            for p in ms:\n",
    "                print(p[\"content\"])\n",
    "        print(\"\\n\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b7a885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Responses\n",
    "for g in game_logs:\n",
    "    print(f'{g[\"game_result\"]}  |  Ch: {str(g[\"chameleon_index\"]+1)}  |  VCh: {str(g[\"voted_chameleon\"])}')\n",
    "    print(f'  secret: {g[\"secret_word\"]}\\n  responses: {g[\"word_responses\"]}\\n  votes: {g[\"votes\"]}')\n",
    "    print(\"  possible: \" + \", \".join(g[\"possible_words\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34543471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Messages\n",
    "for g in game_logs:\n",
    "    for m in g[\"messages\"][2]:\n",
    "        print(m[\"content\"])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868cd15f",
   "metadata": {},
   "source": [
    "## Basic gameplay statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5771e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for plotting\n",
    "chameleon_types = [matchup[0] for matchup in all_games.keys()]\n",
    "chameleon_types = sorted(list(set(chameleon_types)))\n",
    "truthful_types = [matchup[1] for matchup in all_games.keys()]\n",
    "truthful_types = sorted(list(set(truthful_types)))\n",
    "\n",
    "# Construct tables for plotting\n",
    "valid_table = np.zeros((len(chameleon_types), len(truthful_types)))\n",
    "win_table = np.zeros((len(chameleon_types), len(truthful_types)))\n",
    "identification_table = np.zeros((len(chameleon_types), len(truthful_types)))\n",
    "second_stage_win_table = np.zeros((len(chameleon_types), len(truthful_types)))\n",
    "for i in range(len(chameleon_types)):\n",
    "    for j in range(len(truthful_types)):\n",
    "        games = all_games[(chameleon_types[i], truthful_types[j])]\n",
    "        valid_table[len(chameleon_types) - 1 - i][j] = games['num_of_valid_trials']/games['num_of_trials']\n",
    "        win_table[len(chameleon_types) - 1 - i][j] = games['num_of_chameleon_loses']/games['num_of_valid_trials']\n",
    "        identification_table[len(chameleon_types) - 1 - i][j] = games['num_of_chameleon_identified']/games['num_of_valid_trials']\n",
    "        second_stage_win_table[len(chameleon_types) - 1 - i][j] = 1- games['num_of_chameleon_loses']/games['num_of_chameleon_identified']\n",
    "\n",
    "# Round to 2 decimal places\n",
    "valid_table = np.round(valid_table, 2)\n",
    "win_table = np.round(win_table, 2)\n",
    "identification_table = np.round(identification_table, 2)\n",
    "second_stage_win_table = np.round(second_stage_win_table, 2)\n",
    "\n",
    "# Set labels for plotting\n",
    "chameleon_types_short = ['Qwen3 32B AWQ']\n",
    "chameleon_types_short = chameleon_types_short[::-1]\n",
    "truthful_types_short = ['Qwen3 32B AWQ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c1bb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "sns.set_theme()\n",
    "fontsize = 20\n",
    "\n",
    "# Valid games ratio\n",
    "plt.figure(figsize=(4, 4))\n",
    "ax = sns.heatmap(valid_table, annot=True, yticklabels=chameleon_types_short, xticklabels=truthful_types_short,\n",
    "vmin=0, vmax=1, square=True, annot_kws={\"size\": fontsize})\n",
    "ax.set_title(\"Valid Games Ratio\")\n",
    "ax.set(ylabel=\"Chameleon type\", xlabel=\"Non-chameleon type\")\n",
    "plt.subplots_adjust(left=-0, right=1, top=0.9, bottom=0.4)\n",
    "plt.show()\n",
    "\n",
    "# Non-chameleon win ratio\n",
    "plt.figure(figsize=(4, 4))\n",
    "ax = sns.heatmap(win_table, annot=True, yticklabels=chameleon_types_short, xticklabels=truthful_types_short,\n",
    "vmin=0, vmax=1, square=True, annot_kws={\"size\": fontsize})\n",
    "ax.set_title(\"Non-Chameleon Win Ratio\")\n",
    "ax.set(ylabel=\"Chameleon type\", xlabel=\"Non-chameleon type\")\n",
    "plt.subplots_adjust(left=-0, right=1, top=0.9, bottom=0.4)\n",
    "plt.show()\n",
    "\n",
    "# Chameleon identification ratio\n",
    "plt.figure(figsize=(4, 4))\n",
    "ax = sns.heatmap(identification_table, annot=True, yticklabels=chameleon_types_short, xticklabels=truthful_types_short,\n",
    "vmin=0, vmax=1, square=True, annot_kws={\"size\": fontsize})\n",
    "ax.set_title(\"Identification Ratio\")\n",
    "ax.set(ylabel=\"Chameleon type\", xlabel=\"Non-chameleon type\")\n",
    "plt.subplots_adjust(left=-0, right=1, top=0.9, bottom=0.4)\n",
    "plt.show()\n",
    "\n",
    "# Second chance win ratio\n",
    "plt.figure(figsize=(4, 4))\n",
    "ax = sns.heatmap(second_stage_win_table, annot=True, yticklabels=chameleon_types_short, xticklabels=truthful_types_short,\n",
    "vmin=0, vmax=1, square=True, annot_kws={\"size\": fontsize})\n",
    "ax.set_title(\"Second Chance Win Ratio\")\n",
    "ax.set(ylabel=\"Chameleon type\", xlabel=\"Non-chameleon type\")\n",
    "plt.subplots_adjust(left=-0, right=1, top=0.9, bottom=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79d423c",
   "metadata": {},
   "source": [
    "## Evaluator LLM's accuracy in guessing the secret word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58281388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global config\n",
    "cfg = {\n",
    "    \"seed\": 0,\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"run_time\": datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd08db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg[\"eval\"] = {\n",
    "    ### General configuration\n",
    "    \"include_chameleon_response\": False,\n",
    "    \"remap_player_ids_to_be_increasing\": True,\n",
    "    \"free_generation\": True,\n",
    "    \"collect_logprobs_of_all_secret_words\": False,\n",
    "    \"collect_logprobs_of_capitalized_secret_words\": False,\n",
    "    \"all_other_player_idxs\": None, # Set later\n",
    "\n",
    "    ### Evaluation talker configuration\n",
    "    \"cls\": OpenaiTalker,\n",
    "    \"kwargs\": {\n",
    "        \"model_id\": {\n",
    "            \"GPT-4o mini\": \"gpt-4o-mini-2024-07-18\",\n",
    "            \"GPT-4o\": \"gpt-4o-2024-08-06\",\n",
    "            \"GPT-4.1\": \"gpt-4.1-2025-04-14\",\n",
    "            \"GPT-5\": \"gpt-5-2025-08-07\",\n",
    "        }[(model_name := \"GPT-4.1\")], # Change this to your model of choice !\n",
    "        \"api_key\": os.environ[\"OPENAI_API_KEY\"],\n",
    "        \"start_conversation\": False,\n",
    "        \"additional_generation_kwargs\": {\n",
    "            \"GPT-4o mini\": {\n",
    "                \"max_output_tokens\": 20,\n",
    "                \"temperature\": 0,\n",
    "            },\n",
    "            \"GPT-4o\": {\n",
    "                \"max_output_tokens\": 20,\n",
    "                \"temperature\": 0,\n",
    "            },\n",
    "            \"GPT-4.1\": {\n",
    "                \"max_output_tokens\": 20,\n",
    "                \"temperature\": 0,\n",
    "            },\n",
    "            \"GPT-5\": {\n",
    "                \"max_output_tokens\": 5000,\n",
    "                \"temperature\": 1,\n",
    "                \"reasoning\": {\"effort\": \"low\"},\n",
    "            },\n",
    "        }[model_name],\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf02dcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the player indices (0-based indexing) - Corresponds to sets of player indices for whose response words to collect secret word predictions\n",
    "player_set_idxs = [\n",
    "    (0,),\n",
    "    (1,),\n",
    "    (2,),\n",
    "    (0, 1),\n",
    "    (0, 2),\n",
    "    (1, 2),\n",
    "    (0, 1, 2),\n",
    "]\n",
    "# Generate all possible permutations of the player sets\n",
    "seed_all(cfg[\"seed\"])\n",
    "num_orig_player_sets = len(player_set_idxs)\n",
    "cfg[\"eval\"][\"all_other_player_idxs\"] = []\n",
    "for player_set_i in range(num_orig_player_sets):\n",
    "    player_set = player_set_idxs[player_set_i]\n",
    "    cfg[\"eval\"][\"all_other_player_idxs\"].append(player_set)\n",
    "\n",
    "    if len(player_set) > 1:\n",
    "        # Add a random permutation of the player set\n",
    "        while (player_set_permutation := tuple(random.sample(player_set, len(player_set)))) == player_set:\n",
    "            continue\n",
    "        cfg[\"eval\"][\"all_other_player_idxs\"].append(player_set_permutation)\n",
    "\n",
    "print(f\"Generated {len(cfg['eval']['all_other_player_idxs'])} player set permutations from {num_orig_player_sets} original player sets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e173deb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the evaluation talker\n",
    "seed_all(cfg[\"seed\"])\n",
    "evaluator = cfg[\"eval\"][\"cls\"](**cfg[\"eval\"][\"kwargs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdea7958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect chameleon's guesses about the secret word\n",
    "seed_all(cfg[\"seed\"])\n",
    "llm_preds_at_bpoints = collect_llm_preds_at_breakpoints(\n",
    "    game_logs=game_logs,\n",
    "    model=evaluator,\n",
    "    tokenizer=None,\n",
    "    token_idx=0, # not used\n",
    "    layer_idx=0, # not used\n",
    "    generation_kwargs=None, # not used\n",
    "    verbose=True,\n",
    "    all_other_player_idxs=cfg[\"eval\"][\"all_other_player_idxs\"],\n",
    "    remap_player_ids_to_be_increasing=cfg[\"eval\"][\"remap_player_ids_to_be_increasing\"], # When all_other_player_idxs contain decreasing sequences, remap the player IDs in the prompt to be increasing \n",
    "    include_chameleon_response=cfg[\"eval\"][\"include_chameleon_response\"],\n",
    "    free_generation=cfg[\"eval\"][\"free_generation\"],\n",
    "    collect_logprobs_of_all_secret_words=cfg[\"eval\"][\"collect_logprobs_of_all_secret_words\"],\n",
    "    collect_logprobs_of_capitalized_secret_words=cfg[\"eval\"][\"collect_logprobs_of_capitalized_secret_words\"]\n",
    ")\n",
    "\n",
    "# Save the collected evaluators' predictions\n",
    "save_to = os.path.join(os.environ[\"DATA_DIR\"], \"chameleon\", f\"evaluation_{cfg['run_time']}.pkl\")\n",
    "with open(save_to, \"wb\") as f:\n",
    "    pickle.dump({\n",
    "        \"config\": cfg,\n",
    "        \"predictions\": llm_preds_at_bpoints\n",
    "    }, f)\n",
    "print(f\"Saved predictions to\\n{save_to}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c0fd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-collected evaluators' predictions\n",
    "load_from = os.path.join(os.environ[\"DATA_DIR\"], \"chameleon\", f\"evaluation_2025-08-16_13-04.pkl\")\n",
    "with open(load_from, \"rb\") as f:\n",
    "    saved_data = pickle.load(f)\n",
    "cfg = saved_data[\"config\"]\n",
    "llm_preds_at_bpoints = saved_data[\"predictions\"]\n",
    "print(f\"Loaded predictions from\\n{load_from}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199b016e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect evaluator's prediction and GT secret words\n",
    "preds = collections.defaultdict(list)\n",
    "gts = collections.defaultdict(list)\n",
    "\n",
    "for player_id_set, evaluated_games in zip(cfg[\"eval\"][\"all_other_player_idxs\"], llm_preds_at_bpoints):\n",
    "    # player_id_set is a tuple of player IDs (0-based indexing)\n",
    "    player_id_set = tuple(player_id_set)\n",
    "\n",
    "    for eval_game in evaluated_games:\n",
    "        assert eval_game[\"other_player_idxs\"] == player_id_set, \\\n",
    "            f\"Expected {player_id_set}, but got {eval_game['other_player_idxs']}\"\n",
    "        preds[player_id_set].append(eval_game[\"response\"])\n",
    "        game = game_logs[eval_game[\"game_index\"]]\n",
    "        gts[player_id_set].append(game[\"secret_word\"])\n",
    "preds, gts = dict(preds), dict(gts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ac795e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the accuracy of the evaluator guessing the correct secret word\n",
    "\n",
    "# Config\n",
    "fontsize = 22\n",
    "add_annots = False\n",
    "save_to = None\n",
    "# save_to = \"evaluator_accuracy_original_vs_permuted.pdf\"\n",
    "\n",
    "\n",
    "# GROUP RESULTS BY LENGTH AND ORDER TYPE (ORIGINAL VS. PERMUTED)\n",
    "results_original = collections.defaultdict(list)\n",
    "results_permuted = collections.defaultdict(list)\n",
    "for player_id_set, predictions in preds.items():\n",
    "    num_words = len(player_id_set)\n",
    "    ground_truths = gts[player_id_set]\n",
    "    \n",
    "    # Check if the order is original (e.g., (0, 1, 2)) or permuted\n",
    "    is_original_order = (tuple(player_id_set) == tuple(range(num_words)))\n",
    "    \n",
    "    # Check correctness for each prediction in this group\n",
    "    for pred, gt in zip(predictions, ground_truths):\n",
    "        is_correct = gt.lower() == pred.lower()\n",
    "        if is_original_order:\n",
    "            results_original[num_words].append(is_correct)\n",
    "        else:\n",
    "            results_permuted[num_words].append(is_correct)\n",
    "\n",
    "# CALCULATE ACCURACY FOR EACH GROUP\n",
    "# --- Original Order ---\n",
    "sorted_lengths_orig = sorted(results_original.keys())\n",
    "accuracies_orig = [np.mean(results_original[length]) for length in sorted_lengths_orig]\n",
    "\n",
    "# --- Permuted Orders ---\n",
    "sorted_lengths_perm = sorted(results_permuted.keys())\n",
    "accuracies_perm = [np.mean(results_permuted[length]) for length in sorted_lengths_perm]\n",
    "\n",
    "# PLOT THE ACCURACY TRENDS\n",
    "sns.set_theme(style=\"whitegrid\", context=\"talk\")\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Plot the line for the original order\n",
    "ax = sns.lineplot(\n",
    "    x=sorted_lengths_orig,\n",
    "    y=accuracies_orig,\n",
    "    marker='o',\n",
    "    markersize=10,\n",
    "    linewidth=3,\n",
    "    color='royalblue',\n",
    "    label='Original order'\n",
    ")\n",
    "\n",
    "if add_annots:\n",
    "    # Add text annotations for the original order line\n",
    "    for x, y in zip(sorted_lengths_orig, accuracies_orig):\n",
    "        ax.text(x, y + 0.03, f\"{y:.1%}\", ha='center', fontsize=fontsize, fontweight='bold', color='royalblue')\n",
    "\n",
    "# Plot the line for the permuted orders\n",
    "if accuracies_perm:\n",
    "    sns.lineplot(\n",
    "        x=sorted_lengths_perm,\n",
    "        y=accuracies_perm,\n",
    "        marker='s',  # Use a square marker to differentiate\n",
    "        markersize=10,\n",
    "        linewidth=3,\n",
    "        color='orangered',\n",
    "        label='Permuted order'\n",
    "    )\n",
    "    if add_annots:\n",
    "        # Add text annotations for the permuted order line\n",
    "        for x, y in zip(sorted_lengths_perm, accuracies_perm):\n",
    "            ax.text(x, y - 0.08, f\"{y:.1%}\", ha='center', fontsize=fontsize, fontweight='bold', color='orangered')\n",
    "\n",
    "# --- Make it nicer: Add titles, labels, and fine-tune aesthetics ---\n",
    "ax.set_xlabel(\"Number of response words\", fontsize=fontsize, labelpad=15)\n",
    "ax.set_ylabel(\"Accuracy\", fontsize=fontsize, labelpad=15)\n",
    "\n",
    "# Set axis limits and ticks to encompass all data\n",
    "all_lengths = sorted(list(set(sorted_lengths_orig + sorted_lengths_perm)))\n",
    "ax.set_ylim(0.5, 1.01)\n",
    "if all_lengths:\n",
    "    ax.set_xticks(all_lengths)\n",
    "ax.tick_params(axis='both', which='major', labelsize=fontsize)\n",
    "\n",
    "# Add a subtle grid and a legend\n",
    "ax.grid(True, which='major', linestyle='--', linewidth=0.6, alpha=0.6)\n",
    "ax.legend(fontsize=fontsize, loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "if save_to is not None:\n",
    "    plt.savefig(save_to)\n",
    "    print(f\"Saved figure to {save_to}\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
